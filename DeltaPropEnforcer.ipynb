{"cells":[{"cell_type":"markdown","source":["# Batch ALTER Lakehouse Tables ‚Äì **Rollback Safe**\n","\n","**Goal:** Apply Delta properties **safely** with **100% reliable rollback** (other than internal Delta properties).\n","\n"," **Key Features**\n"," - **Safe property order** (reader/writer first)\n"," - **Idempotent apply** (skip if already set)\n"," - **Smart dry-run**\n"," - **Absolute rollback** ‚Äî **only user-managed props**\n"," - **Tiny, safe backups** (per lakehouse)\n"," - **No internal Delta props** (`maxColumnId`, etc.)\n"," - **Persistent logs** in `Files/logs/`\n"," - **Graceful skip** on active writes\n","\n","---\n","\n","### Why This Notebook?\n","This notebook safely modifies Delta table properties in Microsoft Fabric Lakehouses. It ensures changes are applied without risking data loss, supports previews (dry-runs), skips tables with ongoing writes to prevent conflicts, and provides full rollback capability.\n","\n","- **Why dry-runs?** A dry-run simulates changes without actually applying them. This lets you preview what would happen (e.g., which properties would change) to avoid mistakes in production.\n","- **Why skip if something is writing to a table?** If a table is being modified (e.g., by another process inserting data), altering its properties could cause conflicts or corruption. Skipping ensures safety; you can retry later.\n","- **Why rollback?** Changes might have unintended effects (e.g., performance issues). Rollback reverts to the previous state using backups, providing a safety net.\n","- **Why wait to check if successful?** Metadata changes (like table properties) might not reflect immediately due to system delays or caching. Waiting and polling ensures the change was applied correctly before proceeding.\n","- **Why and where save log files?** Logs record every action for auditing, debugging, and compliance. They are saved persistently in each lakehouse under `Files/logs/` (a folder in the lakehouse storage) so they survive notebook sessions and are organized per lakehouse for easy access.\n"],"metadata":{},"id":"5eeda445-39e9-472a-b910-6725956b3393"},{"cell_type":"code","source":["# We need semantic link to scan workspaces to find all lakehouses for batch ALTER\n","%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true},"editable":true,"run_control":{"frozen":false}},"id":"e75f7653-2a10-4f89-a56e-92a655f63856"},{"cell_type":"code","source":["# Import required libraries: sempy_labs for Fabric interactions, datetime for timestamps, etc.\n","import sempy_labs as labs\n","import sempy.fabric as fabric\n","import urllib.parse\n","from datetime import datetime\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import time\n","import logging\n","from functools import wraps\n","import io\n","from threading import Lock\n","from delta.tables import DeltaTable\n","\n","# === SILENCE CHATTY LIBRARIES ===\n","# Set logging levels to WARNING to reduce unnecessary output from libraries like azure and msal.\n","logging.getLogger(\"azure\").setLevel(logging.WARNING)\n","logging.getLogger(\"sempy\").setLevel(logging.WARNING)\n","logging.getLogger(\"msal\").setLevel(logging.WARNING)\n","logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n","\n","print(\"Libraries loaded. Noisy logs silenced.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b0182c5-0b50-4b41-aa60-7704e5956c86"},{"cell_type":"markdown","source":["## 2. Configuration (Edit Here)\n","\n","This section defines user-editable settings. Edit variables like `workspace`, `MODE`, and `properties` to customize behavior.\n","\n","For developers new to delta lake: `properties` are Delta table settings (e.g., file size for optimization). The backup table stores snapshots before changes for rollback."],"metadata":{},"id":"cbba0470-84af-4c01-9391-69d226a22732"},{"cell_type":"code","source":["# Workspace name in Fabric where lakehouses are located.\n","workspace = \"Local Ambulatory Services\"\n","# Mode: 'dry-run' to preview, 'apply' to make changes, 'rollback' to revert.\n","MODE = \"dry-run\"\n","# Extra DRY_RUN variable for rollback mode only, set to True to preview rollback without applying. Set to False only to apply rollback.\n","DRY_RUN = True\n","# Timestamp from a previous run to rollback to (get from logs or backups). e.g. \"20251030_031558\" or None\n","ROLLBACK_TIMESTAMP = '20251118_144632'\n","\n","# If set, process only this lakehouse; otherwise, enter None to alter all lakehouses in the workspace.\n","#LAKEHOUSE_FILTER = None\n","LAKEHOUSE_FILTER = [\"LocalAmbulatoryServicesTransactionDetail\"]   # ‚Üê Uncomment to target one or more\n","\n","# Table in the default lakehouse to store property backups from all lakehouses.\n","backup_table = \"table_property_backups\"  # Stores backups from ALL managed workspaces/lakehouses\n","\n","# List of Delta properties to set, in a safe order (reader before writer versions).\n","properties = [\n","    #('delta.minReaderVersion', '2'),\n","    #('delta.minWriterVersion', '5'),\n","    #('delta.columnMapping.mode', 'name'),\n","    #('delta.targetFileSize', '128m'),\n","    ('delta.autoOptimize.autoCompact', 'True')\n","]\n","\n","# === INTERNAL DELTA PROPERTIES (DO NOT BACK UP OR RESTORE) ===\n","# Complete list of Delta Lake-managed internal properties\n","INTERNAL_PROPERTIES = {\n","    # Column Mapping (auto-managed by Delta)\n","    'delta.columnMapping.maxColumnId',\n","    'delta.columnMapping.maxColumnNameLength',\n","    \n","    # Protocol Versions (auto-upgraded, never downgrade)\n","    'delta.minReaderVersion',\n","    'delta.minWriterVersion',\n","    \n","    # Checkpoint Internals (auto-managed)\n","    'delta.checkpoint.writeStatsAsJson',\n","    'delta.checkpoint.writeStatsAsStruct',\n","    \n","    # Add more as discovered\n","}\n","\n","# These can be set to 'true' but never back to 'false'\n","ONE_WAY_PROPERTIES = {\n","    'delta.enableChangeDataFeed',\n","    'delta.enableDeletionVectors',\n","}\n","\n","# Valid protocol versions for safety checks\n","VALID_READER_VERSIONS = {'1', '2', '3'}\n","VALID_WRITER_VERSIONS = {'2', '3', '4', '5', '6', '7'}\n","\n","# === PROPERTY VALIDATION RULES ===\n","PROPERTY_RULES = {\n","    'delta.minReaderVersion': {\n","        'valid_values': VALID_READER_VERSIONS,\n","        'allow_downgrade': False,\n","        'warning': 'Reader version upgrades are irreversible. Downgrading will break table compatibility.'\n","    },\n","    'delta.minWriterVersion': {\n","        'valid_values': VALID_WRITER_VERSIONS,\n","        'allow_downgrade': False,\n","        'warning': 'Writer version upgrades are irreversible. Downgrading will corrupt data integrity.'\n","    },\n","    'delta.columnMapping.mode': {\n","        'valid_values': {'none', 'name', 'id'},\n","        'allow_downgrade': False,\n","        'warning': 'Column mapping mode changes are irreversible. Cannot change back to \"none\" once set.'\n","    },\n","    'delta.targetFileSize': {\n","        'valid_values': None,  # Numeric validation\n","        'range': (8, 2048),  # Min 8MB, max 2GB\n","        'warning': 'Target file size should be between 8MB and 2GB for optimal performance.'\n","    }\n","}\n","\n","# === TUNING ===\n","# Maximum tables to process in parallel for efficiency.\n","MAX_PARALLEL_TABLES = 3\n","# Number of retry attempts for failed operations.\n","MAX_RETRIES = 2\n","# Base wait time between retries (exponential backoff).\n","RETRY_WAIT_SECONDS = 5\n","# Minutes to look back for recent writes to detect activity.\n","ACTIVE_WRITE_LOOKBACK_MINUTES = 1\n","\n","# === VALIDATION ===\n","# Timeout in seconds to wait for property validation.\n","VALIDATION_TIMEOUT_SECONDS = 90\n","# Interval in seconds to poll during validation.\n","VALIDATION_POLL_INTERVAL = 3\n","\n","# === FIX DELTA DETECTION ===\n","spark.conf.set(\"spark.microsoft.delta.formatCheck.enabled\", \"false\")\n","spark.conf.set(\"spark.sql.sources.commitProtocolClass\",\n","               \"com.microsoft.delta.commit.CommitProtocol\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"92578271-8b51-4c7b-a4c7-8b9f7cfad352"},{"cell_type":"markdown","source":["## 3. Persistent Logging\n","\n","Sets up logging to capture operations. Logs are saved to files in each lakehouse for persistence and printed in cell output.\n","\n","Logs help track what happened during runs. Saved in default lakehouse under `Files/logs/[workspace]/[lakehouse]/[date]/\"\"`"],"metadata":{},"id":"2687db00-c1c2-4b36-ab9e-959dc1e21b95"},{"cell_type":"code","source":["# Generate a timestamp for unique log filenames.\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Build mode string that includes dry-run status for rollback\n","if MODE == \"rollback\":\n","    log_mode = f\"rollback-dryrun\" if DRY_RUN else \"rollback\"\n","else:\n","    log_mode = MODE\n","\n","log_filename = f\"logs/table_property_update_{log_mode}_{timestamp}.log\"\n","\n","# Check if running in Spark environment (mssparkutils is Fabric-specific).\n","if 'mssparkutils' in globals():\n","    log_dir = \"Files/logs\"\n","    if not mssparkutils.fs.exists(log_dir):\n","        mssparkutils.fs.mkdirs(log_dir)\n","    log_path = f\"/lakehouse/default/Files/{log_filename}\"\n","else:\n","    log_path = log_filename\n","\n","# Configure logging: Set level, format, and handlers (file and console).\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s [%(levelname)s] %(message)s',\n","    handlers=[\n","        logging.FileHandler(log_path),\n","        logging.StreamHandler()\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","print(f\"Log: Files/{log_filename}\")\n","\n","# === PER-LAKEHOUSE LOGGING ===\n","log_buffers = {}  # {lakehouse: StringIO buffer}\n","\n","# Function to save in-memory logs to lakehouse files.\n","def flush_logs_to_lakehouse(run_timestamp: str):  # ‚Üê Accept timestamp parameter\n","    \"\"\"Save logs to default lakehouse under Files/logs/[workspace]/[lakehouse]/[date]/\"\"\"\n","    \n","    current_log_mode = log_mode\n","    base_dir = \"Files/logs\"\n","    \n","    try:\n","        mssparkutils.fs.mkdirs(base_dir)\n","    except Exception as e:\n","        print(f\"ERROR creating base directory: {e}\")\n","        return\n","\n","    saved_count = 0\n","    \n","    for lh, buffer in log_buffers.items():\n","        try:\n","            content = buffer.getvalue().strip()\n","            \n","            if not content:\n","                continue\n","            \n","            # USE THE PASSED TIMESTAMP instead of creating new one\n","            date_folder = datetime.now().strftime(\"%Y-%m-%d\")\n","            filename = f\"{run_timestamp}_{current_log_mode}_{lh}.log\"  # ‚Üê Use run_timestamp\n","            log_path = f\"{base_dir}/{workspace}/{lh}/{date_folder}/{filename}\"\n","            \n","            dir_path = f\"{base_dir}/{workspace}/{lh}/{date_folder}\"\n","            mssparkutils.fs.mkdirs(dir_path)\n","            \n","            mssparkutils.fs.put(log_path, content, True)\n","            print(f\"LOG SAVED: {log_path}\")\n","            saved_count += 1\n","            \n","        except Exception as e:\n","            print(f\"ERROR saving log for '{lh}': {e}\")\n","    \n","    print(f\"FLUSH COMPLETED: {saved_count}/{len(log_buffers)} logs saved\")\n","    \n","    for buffer in log_buffers.values():\n","        buffer.close()\n","    log_buffers.clear()\n","\n","def get_lakehouse_logger(lh: str):\n","    \"\"\"Returns a logger that writes to in-memory buffer\"\"\"\n","    if lh not in log_buffers:\n","        log_buffers[lh] = io.StringIO()\n","    \n","    logger = logging.getLogger(f\"lakehouse_{lh}\")\n","    logger.setLevel(logging.INFO)\n","    logger.handlers.clear()\n","    \n","    buffer_handler = logging.StreamHandler(log_buffers[lh])\n","    buffer_handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))\n","    logger.addHandler(buffer_handler)\n","    \n","    return logger"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d226acd9-f7d3-40c7-be56-59a7f3ddd113"},{"cell_type":"markdown","source":["## 4. Helpers\n","\n","Utility functions for common tasks like quoting identifiers, retries, and fetching table properties.\n","\n","Retries handle transient errors (e.g., network issues). OneLake paths allow accessing tables across workspaces without attaching lakehouses."],"metadata":{},"id":"a6b91fa9-2d4f-4b27-a5b1-4a89548eaa4f"},{"cell_type":"code","source":["def normalize_property_value(key: str, value: str) -> str:\n","    \"\"\"\n","    Normalize property values for consistent comparison.\n","    Handles boolean case sensitivity and other common variations.\n","    \"\"\"\n","    if value is None:\n","        return None\n","    \n","    # Convert to string if not already\n","    value_str = str(value).strip()\n","    \n","    # Boolean properties - normalize to lowercase\n","    boolean_properties = {\n","        'delta.autoOptimize.autoCompact',\n","        'delta.autoOptimize.optimizeWrite',\n","        'delta.enableChangeDataFeed',\n","        'delta.enableDeletionVectors',\n","        'delta.checkpoint.writeStatsAsJson',\n","        'delta.checkpoint.writeStatsAsStruct'\n","    }\n","    \n","    if key in boolean_properties:\n","        return value_str.lower()  # 'True' -> 'true', 'False' -> 'false'\n","    \n","    # File size properties - normalize to lowercase for 'm', 'mb', 'gb'\n","    if 'FileSize' in key or 'fileSize' in key:\n","        return value_str.lower()  # '128M' -> '128m', '1GB' -> '1gb'\n","    \n","    return value_str"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4af927be-fd00-4d4c-87a7-afbac730c2ab"},{"cell_type":"code","source":["def validate_property(key: str, new_value: str, current_value: str = None, strict_mode: bool = True) -> tuple[bool, str]:\n","    \"\"\"\n","    Validate if a property change is safe.\n","    \n","    Args:\n","        key: Property name\n","        new_value: New value to set\n","        current_value: Current value (if any)\n","        strict_mode: If True, block unsafe changes; if False, warn only\n","    \n","    Returns:\n","        (is_valid, message)\n","    \"\"\"\n","    \n","    # Check 1: Block internal properties\n","    if key in INTERNAL_PROPERTIES:\n","        return False, f\"‚ùå BLOCKED: '{key}' is an internal Delta property and cannot be manually modified.\"\n","    \n","    # Check 2: Validate against rules\n","    if key in PROPERTY_RULES:\n","        rule = PROPERTY_RULES[key]\n","        \n","        # Validate against allowed values (use .get() for safety)\n","        if rule.get('valid_values') and new_value not in rule['valid_values']:\n","            return False, f\"‚ùå INVALID: '{key}' must be one of {rule['valid_values']}, got '{new_value}'\"\n","        \n","        # Check for downgrades (use .get() for safety)\n","        if rule.get('allow_downgrade') == False and current_value:\n","            try:\n","                # For version numbers, compare numerically\n","                if 'Version' in key:\n","                    if int(new_value) < int(current_value):\n","                        warning = rule.get('warning', 'Downgrade detected')\n","                        msg = f\"‚ö†Ô∏è DANGEROUS: Downgrading {key} from '{current_value}' to '{new_value}'\\n   {warning}\"\n","                        if strict_mode:\n","                            return False, f\"‚ùå BLOCKED: {msg}\"\n","                        else:\n","                            return True, f\"‚ö†Ô∏è WARNING: {msg}\"\n","            except ValueError:\n","                pass  # Not numeric, skip comparison\n","        \n","        # Range validation for numeric properties\n","        if 'range' in rule:\n","            try:\n","                value_int = int(new_value)\n","                min_val, max_val = rule['range']\n","                if not (min_val <= value_int <= max_val):\n","                    return False, f\"‚ùå OUT OF RANGE: '{key}' must be between {min_val} and {max_val}, got {value_int}\"\n","            except ValueError:\n","                return False, f\"‚ùå INVALID: '{key}' must be a number, got '{new_value}'\"\n","    \n","    # Check 3: One-way properties\n","    if key in ONE_WAY_PROPERTIES:\n","        if current_value == 'true' and new_value == 'false':\n","            msg = f\"‚ö†Ô∏è IRREVERSIBLE: '{key}' cannot be disabled once enabled\"\n","            if strict_mode:\n","                return False, f\"‚ùå BLOCKED: {msg}\"\n","            else:\n","                return True, f\"‚ö†Ô∏è WARNING: {msg}\"\n","    \n","    return True, f\"‚úÖ Safe to change '{key}' to '{new_value}'\"\n","\n","\n","def validate_properties_list(properties_list: list, strict_mode: bool = True) -> tuple[bool, list]:\n","    \"\"\"\n","    Validate entire properties configuration before execution.\n","    \n","    Args:\n","        properties_list: List of (key, value) tuples\n","        strict_mode: If True, block on any validation failure\n","    \n","    Returns:\n","        (all_valid, messages)\n","    \"\"\"\n","    all_valid = True\n","    messages = []\n","    \n","    for key, value in properties_list:\n","        is_valid, msg = validate_property(key, value, strict_mode=strict_mode)\n","        messages.append(msg)\n","        \n","        if not is_valid:\n","            all_valid = False\n","    \n","    return all_valid, messages\n","\n","\n","# ====================================================================\n","# BACKUP TABLE CREATION\n","# ====================================================================\n","\n","def ensure_backup_table_exists():\n","    \"\"\"Create backup table in default lakehouse if it doesn't exist\"\"\"\n","    try:\n","        spark.sql(f\"DESCRIBE TABLE {backup_table}\")\n","        print(f\"Backup table '{backup_table}' exists in default lakehouse\")\n","    except:\n","        print(f\"Creating backup table '{backup_table}' in default lakehouse...\")\n","        spark.sql(f\"\"\"\n","            CREATE TABLE {backup_table} (\n","                run_timestamp STRING,\n","                execution_time TIMESTAMP,\n","                execution_mode STRING,\n","                backup_timing STRING,\n","                target_workspace STRING,\n","                lakehouse_name STRING,\n","                table_name STRING,\n","                property_key STRING,\n","                property_value STRING\n","            )\n","            USING DELTA\n","            TBLPROPERTIES (\n","                'delta.autoOptimize.autoCompact' = 'true'\n","            )\n","        \"\"\")\n","        print(\"Backup table created successfully\")\n","\n","# Call this at the start of execution\n","ensure_backup_table_exists()\n","\n","\n","# ====================================================================\n","# STARTUP VALIDATION - RUNS AUTOMATICALLY\n","# ====================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"VALIDATING PROPERTIES CONFIGURATION\")\n","print(\"=\"*80)\n","\n","all_valid, validation_messages = validate_properties_list(properties, strict_mode=True)\n","\n","for msg in validation_messages:\n","    print(msg)\n","\n","if not all_valid:\n","    print(\"\\n‚ùå CONFIGURATION CONTAINS UNSAFE PROPERTIES\")\n","    print(\"   Please review and remove blocked properties before running.\")\n","    print(\"=\"*80)\n","    raise ValueError(\"Unsafe properties detected in configuration. Execution halted.\")\n","else:\n","    print(\"\\n‚úÖ All configured properties passed validation\")\n","    print(\"=\"*80)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2ed78b5b-a160-40ce-81b5-4590dec81512"},{"cell_type":"code","source":["# === BUILD ONELAKE PATH FUNCTION ===\n","def build_onelake_path(workspace_name: str, lakehouse_name: str, logger=None) -> str | None:\n","    \"\"\"\n","    Build OneLake ABFSS path using admin scan.\n","    Uses workspace variable from config section.\n","    Logs to both file and console.\n","    \"\"\"\n","    try:\n","        from sempy_labs import admin\n","        \n","        if logger:\n","            logger.info(f\"Building OneLake path for workspace='{workspace_name}' lakehouse='{lakehouse_name}'\")\n","        \n","        # Scan workspace to get IDs\n","        scan_result = admin.scan_workspaces(workspace=workspace_name)\n","        \n","        # Get workspace ID\n","        if not scan_result.get(\"workspaces\") or len(scan_result[\"workspaces\"]) == 0:\n","            msg = f\"Workspace '{workspace_name}' not found\"\n","            if logger:\n","                logger.error(msg)\n","            else:\n","                print(f\"‚ùå {msg}\")\n","            return None\n","        \n","        workspace_id = scan_result[\"workspaces\"][0][\"id\"]\n","        if logger:\n","            logger.info(f\"Found workspace ID: {workspace_id}\")\n","        \n","        # Find lakehouse\n","        lakehouses = scan_result[\"workspaces\"][0].get(\"Lakehouse\", [])\n","        lakehouse = next((lh for lh in lakehouses if lh[\"name\"] == lakehouse_name), None)\n","        \n","        if not lakehouse:\n","            available = [lh[\"name\"] for lh in lakehouses]\n","            msg = f\"Lakehouse '{lakehouse_name}' not found. Available: {available}\"\n","            if logger:\n","                logger.error(msg)\n","            else:\n","                print(f\"‚ùå {msg}\")\n","            return None\n","        \n","        lakehouse_id = lakehouse[\"id\"]\n","        if logger:\n","            logger.info(f\"Found lakehouse ID: {lakehouse_id}\")\n","        \n","        # Build path\n","        path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/\"\n","        \n","        if logger:\n","            logger.info(f\"‚úÖ OneLake path built successfully: {path}\")\n","        else:\n","            print(f\"‚úÖ Built path: {path}\")\n","        \n","        return path\n","        \n","    except Exception as e:\n","        msg = f\"Error building OneLake path: {e}\"\n","        if logger:\n","            logger.error(msg)\n","        else:\n","            print(f\"‚ùå {msg}\")\n","        return None"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dcf20d17-40d8-4c9f-9c09-faec12e98e38"},{"cell_type":"code","source":["# Decorator for retry logic: Wraps functions to retry on failure.\n","def retry_on_failure(max_attempts=MAX_RETRIES, wait_seconds=RETRY_WAIT_SECONDS):\n","    def decorator(func):\n","        @wraps(func)  # Preserves original function's metadata.\n","        def wrapper(*args, **kwargs):\n","            last_e = None\n","            for a in range(1, max_attempts + 1):\n","                try:\n","                    return func(*args, **kwargs)\n","                except Exception as e:\n","                    last_e = e\n","                    if a < max_attempts:\n","                        wait = wait_seconds * (2 ** (a - 1))  # Exponential backoff.\n","                        logger.warning(f\"Retry {a}/{max_attempts} after {wait}s: {e}\")\n","                        time.sleep(wait)  # Pause before retry.\n","            raise last_e\n","        return wrapper\n","    return decorator\n","\n","@retry_on_failure()\n","def get_table_properties_from_path(table_path: str, table_name: str, logger) -> dict:\n","    \"\"\"Get table properties using exact physical path.\"\"\"\n","    logger.debug(f\"Fetching properties for table '{table_name}'\")\n","    \n","    try:\n","        delta_table = DeltaTable.forPath(spark, table_path)\n","        detail = delta_table.detail().collect()[0]\n","        props = dict(detail.properties) if hasattr(detail, 'properties') and detail.properties else {}\n","        logger.debug(f\"Retrieved {len(props)} properties for {table_name}\")\n","        return props\n","    except Exception as e:\n","        logger.warning(f\"Property fetch failed for {table_name}: {e}\")\n","        return {}\n","\n","@retry_on_failure()\n","def get_table_detail_from_path(table_path: str, table_name: str, logger) -> dict:\n","    \"\"\"Get size / file-count using exact physical path.\"\"\"\n","    logger.debug(f\"Inspecting table '{table_name}'\")\n","    \n","    # Quick Delta-check\n","    is_delta = False\n","    try:\n","        mssparkutils.fs.ls(f\"{table_path}/_delta_log\")\n","        is_delta = True\n","    except:\n","        pass\n","    \n","    if not is_delta:\n","        return {'size_bytes': 0, 'num_files': 0, 'format': 'unknown'}\n","\n","    try:\n","        files = mssparkutils.fs.ls(table_path)\n","        parquet_files = [f for f in files if f.name.endswith('.parquet')]\n","        return {\n","            'size_bytes': sum(f.size for f in parquet_files),\n","            'num_files':  len(parquet_files),\n","            'format': 'delta'\n","        }\n","    except Exception as e:\n","        logger.warning(f\"Detail fetch failed for {table_name}: {e}\")\n","        return {'size_bytes': 0, 'num_files': 0, 'format': 'delta'}\n","\n","def check_active_writes_from_path(table_path: str, workspace: str, lakehouse: str, table: str, lookback_minutes: int, logger) -> bool:\n","    \"\"\"Check if table has recent write activity using exact physical path\"\"\"\n","    try:\n","        from datetime import timedelta\n","\n","        cutoff = datetime.now() - timedelta(minutes=lookback_minutes)\n","        \n","        history = spark.sql(f\"DESCRIBE HISTORY delta.`{table_path}` LIMIT 10\").collect()\n","        \n","        for entry in history:\n","            if hasattr(entry, 'timestamp'):\n","                if entry.timestamp > cutoff and entry.operation in ['WRITE', 'MERGE', 'UPDATE', 'DELETE', 'OPTIMIZE']:\n","                    logger.info(f\"{workspace}.{lakehouse}.{table} has recent {entry.operation} at {entry.timestamp}\")\n","                    return True\n","        \n","        return False\n","    except Exception as e:\n","        logger.warning(f\"Could not check active writes for {workspace}.{lakehouse}.{table}: {e}\")\n","        return False\n","\n","@retry_on_failure()\n","def apply_table_property_from_path(table_path: str, table_name: str, k: str, v: str, logger):\n","    \"\"\"Apply a single property using exact physical path.\"\"\"\n","    logger.info(f\"Applying {k} = {v} to table '{table_name}'\")\n","    \n","    try:\n","        spark.sql(f\"ALTER TABLE delta.`{table_path}` SET TBLPROPERTIES ('{k}' = '{v}')\")\n","        logger.info(f\"Property {k} applied successfully\")\n","    except Exception as e:\n","        logger.error(f\"Failed to apply {k} on {table_name}: {e}\")\n","        raise\n","\n","def wait_for_property_from_path(table_path: str, table_name: str, key: str, expected: str, logger,\n","                                timeout=VALIDATION_TIMEOUT_SECONDS,\n","                                interval=VALIDATION_POLL_INTERVAL) -> bool:\n","    \"\"\"Poll until the property matches the expected value.\"\"\"\n","    start = time.time()\n","    \n","    while time.time() - start < timeout:\n","        try:\n","            res = spark.sql(f\"SHOW TBLPROPERTIES delta.`{table_path}`('{key}')\").collect()\n","            if res and len(res) > 0:\n","                actual = res[0][1] if len(res[0]) > 1 else res[0].get('value')\n","                if actual == expected:\n","                    logger.info(f\"{key} validated in {time.time()-start:.1f}s\")\n","                    return True\n","        except Exception as e:\n","            logger.debug(f\"Validation retry: {e}\")\n","        time.sleep(interval)\n","    \n","    logger.warning(f\"Validation TIMEOUT for {key} on {table_name}\")\n","    return False"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"67c34145-257c-4bcf-b4e8-8f51b3485e6d"},{"cell_type":"markdown","source":["## 5. Backup & Rollback (Safe!)\n","\n","Functions for backing up properties before changes and rolling back if needed.\n","\n","Backups are stored in the default lakehouse Delta table for central access. Rollback queries this table to restore old values. Internal properties are skipped to avoid breaking Delta."],"metadata":{},"id":"8ed25241-0997-4faa-9f03-04329066e2e5"},{"cell_type":"code","source":["def save_backup(run_ts, lh, tbl, backup_props, backup_timing, target_workspace):\n","    \"\"\"\n","    Back up to DEFAULT lakehouse (not the table's lakehouse).\n","    backup_props should be a dict of {property_key: current_value}\n","    Only backs up properties that are about to change.\n","    \"\"\"\n","    \n","    # Log what we're backing up\n","    if backup_props:\n","        logger.info(f\"Backing up {len(backup_props)} properties for {lh}.{tbl}: {list(backup_props.keys())}\")\n","    \n","    # Always save to the DEFAULT lakehouse where this notebook runs\n","    backup_fq = f\"`{backup_table}`\"\n","    \n","    try:\n","        # Create rows for properties that are changing\n","        data = [\n","            (run_ts, datetime.now(), MODE, backup_timing, target_workspace, lh, tbl, k, v)\n","            for k, v in backup_props.items()\n","        ]\n","        \n","        if data:\n","            df = spark.createDataFrame(data, [\n","                \"run_timestamp\",\n","                \"execution_time\",\n","                \"execution_mode\",\n","                \"backup_timing\",\n","                \"target_workspace\",\n","                \"lakehouse_name\",\n","                \"table_name\",\n","                \"property_key\",\n","                \"property_value\"\n","            ])\n","            \n","            df.write.mode('append').saveAsTable(backup_fq)\n","            logger.info(f\"Backed up {len(data)} props from {target_workspace}.{lh}.{tbl} to default lakehouse\")\n","        else:\n","            logger.info(f\"No properties to backup for {target_workspace}.{lh}.{tbl}\")\n","            \n","    except Exception as e:\n","        logger.error(f\"Backup failed for {target_workspace}.{lh}.{tbl}: {e}\")\n","\n","# Function to perform rollback.\n","def rollback_to_timestamp(ts: str, target_workspace: str = None, \n","                          lakehouse_filter = None, dry_run: bool = False):\n","    \"\"\"Rollback from backups stored in DEFAULT lakehouse\"\"\"\n","    print(f\"{'[DRY RUN] ' if dry_run else ''}Searching default lakehouse for backup: {ts}\")\n","    \n","    # Pretty print for list vs string\n","    if lakehouse_filter:\n","        if isinstance(lakehouse_filter, list):\n","            print(f\"Filtering to lakehouse(s): {', '.join(lakehouse_filter)}\")\n","        else:\n","            print(f\"Filtering to lakehouse: {lakehouse_filter}\")\n","    \n","    try:\n","        # Query the default lakehouse backup table.\n","        query = f\"SELECT * FROM {backup_table} WHERE run_timestamp = '{ts}'\"\n","        \n","        if target_workspace:\n","            query += f\" AND target_workspace = '{target_workspace}'\"\n","        \n","        # Handle both list and string properly\n","        if lakehouse_filter:\n","            if isinstance(lakehouse_filter, list):\n","                # Handle list: use IN clause\n","                lakehouse_list = \"', '\".join(lakehouse_filter)\n","                query += f\" AND lakehouse_name IN ('{lakehouse_list}')\"\n","            else:\n","                # Handle string: use equality\n","                query += f\" AND lakehouse_name = '{lakehouse_filter}'\"\n","        \n","        data = spark.sql(query).collect()  # Execute query and collect results.\n","        \n","        if data:\n","            unique_lakehouses = set(r.lakehouse_name for r in data)  # Get unique lakehouses.\n","            print(f\"Found {len(data)} property backups across {len(unique_lakehouses)} lakehouse(s): {', '.join(unique_lakehouses)}\")\n","            \n","            if dry_run:\n","                print(\"\\nüîç DRY RUN - Would restore:\")\n","                restore_from_data(data, dry_run=True)\n","            else:\n","                # Ask for confirmation if many properties\n","                if len(data) > 10 and not lakehouse_filter:\n","                    print(f\"‚ö†Ô∏è  WARNING: This will restore {len(data)} properties across {len(unique_lakehouses)} lakehouses\")\n","                    response = input(\"Continue? (yes/no): \")  # User input for confirmation.\n","                    if response.lower() != 'yes':\n","                        print(\"Rollback cancelled\")\n","                        return\n","                \n","                restore_from_data(data, dry_run=False)\n","        else:\n","            if lakehouse_filter:\n","                filter_display = ', '.join(lakehouse_filter) if isinstance(lakehouse_filter, list) else lakehouse_filter\n","                print(f\"No backup found for lakehouse '{filter_display}' at timestamp {ts}\")\n","            else:\n","                print(\"No backup found.\")\n","            list_recent_backups(target_workspace, lakehouse_filter)\n","    except Exception as e:\n","        print(f\"Error reading backup: {e}\")\n","        list_recent_backups(target_workspace, lakehouse_filter)\n","        \n","def restore_from_data(data, dry_run: bool = False):\n","    \"\"\"Restore properties from backup data with comprehensive duplicate detection\"\"\"\n","    \n","    # ========================================\n","    # DIAGNOSTIC LEVEL 1: Check raw backup data\n","    # ========================================\n","    property_counts = {}\n","    for r in data:\n","        key = (r.target_workspace, r.lakehouse_name, r.table_name, r.property_key)\n","        property_counts[key] = property_counts.get(key, 0) + 1\n","    \n","    duplicates = {k: v for k, v in property_counts.items() if v > 1}\n","    if duplicates:\n","        print(\"‚ö†Ô∏è WARNING: Duplicate properties found in backup data:\")\n","        for (ws, lh, tbl, prop), count in duplicates.items():\n","            print(f\"  {ws}.{lh}.{tbl}.{prop}: {count} times\")\n","    \n","    # ========================================\n","    # Group properties by table\n","    # ========================================\n","    tables = {}\n","    for r in data:\n","        if r.property_key in INTERNAL_PROPERTIES:\n","            continue\n","        key = (r.target_workspace, r.lakehouse_name, r.table_name)\n","        tables.setdefault(key, []).append((r.property_key, r.property_value))\n","    \n","    # ========================================\n","    # DIAGNOSTIC LEVEL 2: Check grouped tables\n","    # ========================================\n","    for (ws, lh, tbl), props in tables.items():\n","        prop_keys = [k for k, v in props]\n","        dup_props = [k for k in prop_keys if prop_keys.count(k) > 1]\n","        if dup_props:\n","            print(f\"‚ö†Ô∏è WARNING: {ws}.{lh}.{tbl} has duplicate properties: {set(dup_props)}\")\n","    \n","    # ========================================\n","    # Process each table\n","    # ========================================\n","    for (ws, lh, tbl), props in tables.items():\n","        logger = get_lakehouse_logger(lh)\n","        \n","        # ========================================\n","        # DIAGNOSTIC LEVEL 3: Final pre-restore check\n","        # ========================================\n","        prop_keys = [k for k, v in props]\n","        if len(prop_keys) != len(set(prop_keys)):\n","            dup_keys = [k for k in set(prop_keys) if prop_keys.count(k) > 1]\n","            warning_msg = f\"‚ö†Ô∏è DUPLICATE PROPERTIES DETECTED for {tbl}: {dup_keys}\"\n","            logger.warning(warning_msg)\n","            print(warning_msg)\n","            \n","            # Show all occurrences with their values\n","            for dup_key in dup_keys:\n","                occurrences = [(k, v) for k, v in props if k == dup_key]\n","                logger.warning(f\"  {dup_key} appears {len(occurrences)} times:\")\n","                print(f\"  {dup_key} appears {len(occurrences)} times:\")\n","                for i, (k, v) in enumerate(occurrences, 1):\n","                    logger.warning(f\"    Occurrence {i}: value = '{v}'\")\n","                    print(f\"    Occurrence {i}: value = '{v}'\")\n","        \n","        # ========================================\n","        # Start restore operation\n","        # ========================================\n","        logger.info(f\"{'[DRY RUN] ' if dry_run else ''}Restoring {ws}.{lh}.{tbl}\")\n","        print(f\"{'[DRY RUN] ' if dry_run else ''}Restoring {ws}.{lh}.{tbl}\")\n","        \n","        # ========================================\n","        # Build OneLake path\n","        # ========================================\n","        try:\n","            onelake_path = build_onelake_path(ws, lh, logger)\n","            \n","            if onelake_path:\n","                physical_path = get_table_physical_path(ws, lh, tbl, onelake_path, logger)\n","                \n","                if physical_path:\n","                    current_props = get_table_properties_from_path(physical_path, tbl, logger)\n","                    table_path = physical_path\n","                else:\n","                    logger.warning(f\"Could not resolve physical path for {tbl}\")\n","                    print(f\"   ‚ö†Ô∏è Warning: Could not resolve physical path for {tbl}\")\n","                    current_props = {}\n","                    table_path = f\"{onelake_path}{tbl}\"\n","            else:\n","                logger.warning(f\"Could not build OneLake path for {ws}.{lh}\")\n","                print(f\"   ‚ö†Ô∏è Warning: Could not build OneLake path for {ws}.{lh}\")\n","                current_props = {}\n","                \n","                # LAST RESORT: Try to get GUIDs via scan as fallback\n","                try:\n","                    logger.info(\"Attempting to scan workspace for GUIDs...\")\n","                    print(f\"   üîÑ Attempting to scan workspace for GUIDs...\")\n","                    from sempy_labs import admin\n","                    scan_result = admin.scan_workspaces(workspace=ws)\n","                    \n","                    if scan_result.get(\"workspaces\") and len(scan_result[\"workspaces\"]) > 0:\n","                        workspace_id = scan_result[\"workspaces\"][0][\"id\"]\n","                        lakehouses = scan_result[\"workspaces\"][0].get(\"Lakehouse\", [])\n","                        lakehouse_obj = next((l for l in lakehouses if l[\"name\"] == lh), None)\n","                        \n","                        if lakehouse_obj:\n","                            lakehouse_id = lakehouse_obj[\"id\"]\n","                            table_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/{tbl}\"\n","                            logger.info(\"Built GUID-based path via scan\")\n","                            print(f\"   ‚úÖ Built GUID-based path via scan\")\n","                        else:\n","                            raise Exception(f\"Lakehouse {lh} not found in workspace scan\")\n","                    else:\n","                        raise Exception(f\"Workspace {ws} not found in scan\")\n","                        \n","                except Exception as scan_error:\n","                    logger.error(f\"Could not get GUIDs: {scan_error}\")\n","                    logger.error(f\"CRITICAL: Cannot restore {tbl} - no valid path available\")\n","                    print(f\"   ‚ùå Could not get GUIDs: {scan_error}\")\n","                    print(f\"   ‚ö†Ô∏è CRITICAL: Cannot restore {tbl} - skipping\")\n","                    continue\n","                \n","        except Exception as e:\n","            logger.error(f\"Error during path resolution: {e}\")\n","            logger.error(f\"CRITICAL: Cannot restore {tbl} - skipping\")\n","            print(f\"   ‚ùå Error during path resolution: {e}\")\n","            print(f\"   ‚ö†Ô∏è CRITICAL: Cannot restore {tbl} - skipping\")\n","            continue\n","        \n","        # ========================================\n","        # Execute restore (dry-run or actual)\n","        # ========================================\n","        if dry_run:\n","            for k, v in props:\n","                current_val = current_props.get(k, 'NOT SET')\n","                if v is not None:\n","                    msg = f\"Would SET {k}: '{current_val}' ‚Üí '{v}'\"\n","                    logger.info(msg)\n","                    print(f\"   {msg}\")\n","                else:\n","                    msg = f\"Would UNSET {k} (current: '{current_val}')\"\n","                    logger.info(msg)\n","                    print(f\"   {msg}\")\n","            continue\n","        \n","        # Apply the restore\n","        try:\n","            for k, v in props:\n","                current_val = current_props.get(k, 'NOT SET')\n","                if v is not None:\n","                    spark.sql(f\"ALTER TABLE delta.`{table_path}` SET TBLPROPERTIES ('{k}' = '{v}')\")\n","                    msg = f\"SET {k}: '{current_val}' ‚Üí '{v}'\"\n","                    logger.info(msg)\n","                    print(f\"   ‚úÖ {msg}\")\n","                else:\n","                    spark.sql(f\"ALTER TABLE delta.`{table_path}` UNSET TBLPROPERTIES IF EXISTS ('{k}')\")\n","                    msg = f\"UNSET {k} (was: '{current_val}')\"\n","                    logger.info(msg)\n","                    print(f\"   ‚úÖ {msg}\")\n","            \n","            success_msg = \"SUCCESS - All properties restored\"\n","            logger.info(success_msg)\n","            print(f\"   ‚úÖ {success_msg}\")\n","            \n","        except Exception as e:\n","            error_msg = f\"FAILED to restore properties: {e}\"\n","            logger.error(error_msg)\n","            print(f\"   ‚ùå {error_msg}\")\n","\n","def list_recent_backups(target_workspace: str = None, lakehouse_filter = None):\n","    \"\"\"List recent backups from DEFAULT lakehouse\"\"\"\n","    print(\"\\nRecent backup runs:\")\n","    try:\n","        where_clauses = [\"1=1\"]\n","        \n","        if target_workspace:\n","            where_clauses.append(f\"target_workspace = '{target_workspace}'\")\n","        \n","        if lakehouse_filter:\n","            if isinstance(lakehouse_filter, list):\n","                lakehouse_list = \"', '\".join(lakehouse_filter)\n","                where_clauses.append(f\"lakehouse_name IN ('{lakehouse_list}')\")\n","            else:\n","                where_clauses.append(f\"lakehouse_name = '{lakehouse_filter}'\")\n","        \n","        where_clause = \" AND \".join(where_clauses)\n","        \n","        # Summary query\n","        summary_query = f\"\"\"\n","            SELECT \n","                run_timestamp, \n","                MAX(execution_time) as execution_time, \n","                target_workspace,\n","                COUNT(DISTINCT lakehouse_name) as lakehouse_count,\n","                COUNT(DISTINCT CONCAT(lakehouse_name, '.', table_name)) as table_count,\n","                COUNT(*) as property_count,\n","                MAX(backup_timing) as mode\n","            FROM {backup_table}\n","            WHERE {where_clause}\n","            GROUP BY run_timestamp, target_workspace\n","            ORDER BY execution_time DESC\n","            LIMIT 10\n","        \"\"\"\n","        recent = spark.sql(summary_query).collect()\n","        \n","        if recent:\n","            print(f\"\\n{'Timestamp':<20} {'Date':<12} {'Workspace':<25} {'Lakehouses':<12} {'Tables':<8} {'Properties':<12} {'Mode':<15}\")\n","            print(\"-\" * 120)\n","            \n","            for r in recent:\n","                exec_date = r.execution_time.strftime(\"%Y-%m-%d\")\n","                \n","                print(f\"{r.run_timestamp:<20} {exec_date:<12} {r.target_workspace:<25} {r.lakehouse_count:<12} {r.table_count:<8} {r.property_count:<12} {r.mode:<15}\")\n","            \n","            print(\"\\nüí° To rollback, copy a timestamp and set:\")\n","            print(\"   MODE = 'rollback'\")\n","            print(\"   ROLLBACK_TIMESTAMP = '<timestamp_from_above>'\")\n","            print(\"   DRY_RUN = True  # for preview, False to apply\")\n","            \n","        else:\n","            print(\"  No backups found\")\n","            \n","            if lakehouse_filter:\n","                filter_display = ', '.join(lakehouse_filter) if isinstance(lakehouse_filter, list) else lakehouse_filter\n","                print(f\"\\nüí° Hint: No backups found for lakehouse '{filter_display}'\")\n","            elif target_workspace:\n","                print(f\"\\nüí° Hint: No backups found for workspace '{target_workspace}'\")\n","            else:\n","                print(f\"\\nüí° Hint: No backups found\")\n","            \n","            print(f\"   - Make sure you've run in MODE='apply' to create backups\")\n","            print(f\"   - Backups are only created when properties are actually changed\")\n","            \n","    except Exception as e:\n","        print(f\"  Error listing backups: {e}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bf1f822d-2c09-4e9d-a9b5-e6d6de3a2966"},{"cell_type":"markdown","source":["## 6. Process One Table\n","\n","Logic for handling a single table: Check, apply, validate.\n","\n","Skips non-Delta tables or active ones. In apply mode, backs up first, applies changes idempotently (skips if already set), then validates by waiting/polling."],"metadata":{},"id":"668e9520-25a0-46ec-82db-9c42dce74cec"},{"cell_type":"code","source":["def get_table_physical_path(workspace: str, lakehouse: str, table: str, onelake_path: str, logger) -> str | None:\n","    \"\"\"\n","    Get the exact physical path of a table using OneLake path.\n","    This handles spaces, mixed case, and special characters correctly.\n","    \"\"\"\n","    try:\n","        logger.debug(f\"Looking for table: '{table}' in lakehouse: '{lakehouse}'\")\n","        logger.debug(f\"OneLake base path: {onelake_path}\")\n","        \n","        # Try multiple path variations\n","        path_attempts = [\n","            table,                    # Original name as-is\n","            table.lower(),            # Lowercase\n","            table.replace(' ', '%20') # URL encoded spaces\n","        ]\n","        \n","        for attempt in path_attempts:\n","            table_path = f\"{onelake_path}{attempt}\"\n","            logger.debug(f\"Trying path: {table_path}\")\n","            \n","            # Check if _delta_log exists (confirms it's a Delta table)\n","            try:\n","                delta_log_path = f\"{table_path}/_delta_log\"\n","                mssparkutils.fs.ls(delta_log_path)\n","                logger.info(f\"‚úÖ Found physical path for {lakehouse}.{table}: {table_path}\")\n","                return table_path\n","            except Exception as e:\n","                logger.debug(f\"Path attempt failed: {attempt} - {e}\")\n","                continue\n","        \n","        # If direct attempts fail, list all tables and find case-insensitive match\n","        logger.debug(f\"Direct path attempts failed, listing tables in: {onelake_path}\")\n","        try:\n","            all_items = mssparkutils.fs.ls(onelake_path)\n","            logger.debug(f\"Found {len(all_items)} items in Tables directory\")\n","            \n","            for item in all_items:\n","                item_name = item.name.rstrip('/')\n","                logger.debug(f\"Checking item: '{item_name}' against '{table}'\")\n","                \n","                # Case-insensitive comparison\n","                if item_name.lower() == table.lower():\n","                    table_path = f\"{onelake_path}{item_name}\"\n","                    \n","                    # Verify it's a Delta table\n","                    try:\n","                        mssparkutils.fs.ls(f\"{table_path}/_delta_log\")\n","                        logger.info(f\"‚úÖ Found table with matching name (case variation): '{table}' -> '{item_name}'\")\n","                        logger.info(f\"Physical path: {table_path}\")\n","                        return table_path\n","                    except Exception as delta_check_error:\n","                        logger.debug(f\"Item '{item_name}' is not a Delta table: {delta_check_error}\")\n","                        continue\n","            \n","            logger.error(f\"Could not find table '{table}' in {len(all_items)} items listed\")\n","            \n","        except Exception as list_error:\n","            logger.error(f\"Could not list tables in {lakehouse}: {list_error}\")\n","        \n","        logger.error(f\"Could not find physical path for {lakehouse}.{table}\")\n","        return None\n","        \n","    except Exception as e:\n","        logger.error(f\"Unexpected error getting physical path for {lakehouse}.{table}: {e}\")\n","        return None"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6fb2536-5c99-451c-a210-da8c1a636e01"},{"cell_type":"code","source":["# Function to process a single table.\n","def process_table(info: dict, run_ts: str, mode: str) -> dict:\n","    lh, tbl, logger, ws = info['lakehouse'], info['table'], info['logger'], info['workspace']\n","    physical_path = info['physical_path']\n","    result = {'table': f\"{lh}.{tbl}\", 'status': 'unknown', 'details': []}\n","\n","    try:\n","        logger.info(f\"Processing {lh}.{tbl}\")\n","        result['details'].append(f\"Table: {lh}.{tbl}\")\n","        result['details'].append(f\"Path: {physical_path}\")\n","\n","        detail = get_table_detail_from_path(physical_path, tbl, logger)\n","        \n","        size_gb = detail['size_bytes'] / (1024**3)\n","        result['details'].append(f\"Size: {size_gb:.2f}GB | Files: {detail['num_files']} | Format: {detail['format']}\")\n","\n","        if detail['format'].lower() != 'delta':\n","            result['status'] = 'skipped'\n","            result['details'].append(\"SKIPPED - Not Delta\")\n","            return result\n","\n","        if check_active_writes_from_path(physical_path, ws, lh, tbl, ACTIVE_WRITE_LOOKBACK_MINUTES, logger):\n","            result['status'] = 'skipped'\n","            result['details'].append(f\"SKIPPED - Active writes in last {ACTIVE_WRITE_LOOKBACK_MINUTES} min\")\n","            return result\n","\n","        props = get_table_properties_from_path(physical_path, tbl, logger)\n","\n","        # ----- DRY-RUN -----\n","        if mode == \"dry-run\":\n","            changes = []\n","            blocked = []\n","            \n","            for k, v in properties:\n","                # Normalize for comparison\n","                current_val = props.get(k, 'NOT SET')\n","                current_normalized = normalize_property_value(k, current_val) if current_val != 'NOT SET' else 'NOT SET'\n","                v_normalized = normalize_property_value(k, v)\n","                \n","                if current_normalized != v_normalized:\n","                    # VALIDATE BEFORE SHOWING AS CHANGE\n","                    is_valid, msg = validate_property(k, v_normalized, current_normalized, strict_mode=True)\n","                    \n","                    if is_valid:\n","                        changes.append(f\"   {k}: '{current_val}' to '{v_normalized}'\")\n","                    else:\n","                        blocked.append(f\"   {k}: '{current_val}' to '{v_normalized}' - {msg}\")\n","            \n","            if blocked:\n","                result['details'].extend([f\"DRY RUN - ‚ö†Ô∏è {len(blocked)} change(s) BLOCKED:\"] + blocked)\n","            \n","            if changes:\n","                result['details'].extend([f\"DRY RUN - Would apply {len(changes)} change(s):\"] + changes)\n","            \n","            if not changes and not blocked:\n","                result['details'].append(\"DRY RUN - No changes needed\")\n","            \n","            result['status'] = 'dry-run'\n","\n","        # ----- APPLY -----\n","        elif mode == \"apply\":\n","            applied = 0\n","            skipped = 0\n","            blocked = 0\n","            backup_props = {}  # Collect properties that will actually change\n","            \n","            for k, v in properties:\n","                # Skip internal properties entirely\n","                if k in INTERNAL_PROPERTIES:\n","                    logger.info(f\"Skipping internal property: {k}\")\n","                    continue\n","                \n","                        # Normalize both current and target values for comparison\n","                    cur = props.get(k, 'NOT SET')\n","                    cur_normalized = normalize_property_value(k, cur) if cur != 'NOT SET' else 'NOT SET'\n","                    v_normalized = normalize_property_value(k, v)\n","                    \n","                    if cur_normalized == v_normalized:\n","                        result['details'].append(f\"   {k}: '{cur}' (no change)\")\n","                        skipped += 1\n","                    else:\n","                        # VALIDATE BEFORE APPLYING\n","                        is_valid, msg = validate_property(k, v_normalized, cur_normalized, strict_mode=True)\n","                        \n","                        if not is_valid:\n","                            result['details'].append(f\"   {k}: BLOCKED - {msg}\")\n","                            logger.warning(f\"Blocked unsafe property change for {lh}.{tbl}: {msg}\")\n","                            blocked += 1\n","                            continue\n","                        \n","                        # Add to backup ONLY if we're going to change it\n","                        # Store the current value (or None if not set) for rollback\n","                        backup_props[k] = None if cur == 'NOT SET' else cur\n","                        \n","                        result['details'].append(f\"   {k}: '{cur}' to '{v_normalized}'\")\n","                        apply_table_property_from_path(physical_path, tbl, k, v_normalized, logger)\n","                        applied += 1\n","            \n","            # Save backup ONLY if we actually changed something\n","            if backup_props:\n","                save_backup(run_ts, lh, tbl, backup_props, 'before_changes', ws)\n","            else:\n","                logger.info(f\"No properties changed for {lh}.{tbl} - skipping backup\")\n","            \n","            result['details'].append(f\"Applied: {applied} | Skipped: {skipped} | Blocked: {blocked}\")\n","\n","            # Validation for applied changes only\n","            all_ok = True\n","            for k, v in properties:\n","                # Only validate properties that were actually applied\n","                if k in backup_props:  # Only validate what we changed\n","                    if not wait_for_property_from_path(physical_path, tbl, k, v, logger):\n","                        result['details'].append(f\"   FAILED validation: {k}\")\n","                        all_ok = False\n","                    else:\n","                        result['details'].append(f\"   Verified: {k}\")\n","            \n","            result['status'] = 'success' if all_ok else 'validation_failed'\n","\n","    except Exception as e:\n","        result['status'] = 'failed'\n","        result['details'].append(f\"Error: {e}\")\n","        logger.error(f\"{lh}.{tbl} - {e}\")\n","\n","    return result"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"125db517-c854-4227-9419-6723e255a2e8"},{"cell_type":"markdown","source":["## 7. Main Execution\n","\n","Main logic: Handles rollback or processes lakehouses/tables in parallel.\n","\n","Uses threading for parallel processing to speed up. Locks ensure safe printing.\n","\n","Lists lakehouses/tables, processes them, summarizes results, flushes logs."],"metadata":{},"id":"fc2d4ad4-4c22-4e82-b6ee-97ae44f500af"},{"cell_type":"code","source":["if MODE == \"rollback\":\n","    if not ROLLBACK_TIMESTAMP:\n","        raise ValueError(\"Set ROLLBACK_TIMESTAMP for rollback mode\")\n","    \n","    # Use current timestamp for this rollback operation\n","    rollback_run_timestamp = timestamp  # Created in Section 3\n","    \n","    print(f\"{'[DRY RUN] ' if DRY_RUN else ''}Starting rollback operation\")\n","    print(f\"Rollback run timestamp: {rollback_run_timestamp}\")\n","    print(f\"Restoring from backup: {ROLLBACK_TIMESTAMP}\")\n","    print()\n","\n","    rollback_to_timestamp(ROLLBACK_TIMESTAMP, workspace, LAKEHOUSE_FILTER, dry_run=DRY_RUN)\n","    \n","    print(\"\\n\" + \"=\"*80)\n","    flush_logs_to_lakehouse(rollback_run_timestamp)\n","    print(f\"üìã Rollback logs saved: Files/logs/{workspace}/[lakehouse]/[date]/\")\n","    print(f\"   Rollback run: {rollback_run_timestamp}\")\n","    print(f\"   Restored from backup: {ROLLBACK_TIMESTAMP}\")\n","    print(\"=\"*80)\n","    \n","else:\n","    log_buffers.clear()\n","    \n","    # Ensure backup table exists\n","    ensure_backup_table_exists()\n","    \n","    lakehouses = labs.list_lakehouses(workspace=workspace)\n","    \n","    if LAKEHOUSE_FILTER:\n","        if isinstance(LAKEHOUSE_FILTER, list):\n","            lakehouses = lakehouses[lakehouses['Lakehouse Name'].isin(LAKEHOUSE_FILTER)]\n","            if lakehouses.empty:\n","                raise ValueError(f\"None of the specified lakehouses found: {LAKEHOUSE_FILTER}\")\n","            print(f\"Targeting {len(lakehouses)} lakehouse(es): {', '.join(LAKEHOUSE_FILTER)}\")\n","        else:\n","            lakehouses = lakehouses[lakehouses['Lakehouse Name'] == LAKEHOUSE_FILTER]\n","            if lakehouses.empty:\n","                raise ValueError(f\"Lakehouse '{LAKEHOUSE_FILTER}' not found\")\n","            print(f\"Targeting lakehouse: {LAKEHOUSE_FILTER}\")\n","    else:\n","        print(f\"Targeting all {len(lakehouses)} lakehouses\")\n","\n","    # === INITIALIZE COLLECTIONS ===\n","    all_tables = []\n","    lakehouse_summary = {} \n","\n","    # === BUILD all_tables LIST ===\n","    for _, row in lakehouses.iterrows():\n","        lh = row['Lakehouse Name']\n","        \n","        lakehouse_logger = get_lakehouse_logger(lh)\n","        onelake_path = build_onelake_path(workspace, lh, lakehouse_logger)\n","        \n","        if not onelake_path:\n","            print(f\"‚ö†Ô∏è  Skipping lakehouse '{lh}' - could not build OneLake path\")\n","            continue\n","        \n","        # Initialize summary for this lakehouse\n","        lakehouse_summary[lh] = {\n","            'tables': 0, \n","            'success': 0, \n","            'skipped': 0, \n","            'failed': 0, \n","            'validation_failed': 0,\n","            'dry-run': 0\n","        }\n","\n","        tables = labs.lakehouse.get_lakehouse_tables(lakehouse=lh, workspace=workspace, exclude_shortcuts=True)\n","        \n","        for _, t in tables.iterrows():\n","            table_name = t['Table Name']\n","            \n","            physical_path = get_table_physical_path(workspace, lh, table_name, onelake_path, lakehouse_logger)\n","            \n","            if not physical_path:\n","                lakehouse_logger.warning(f\"Skipping {table_name} - could not resolve physical path\")\n","                continue\n","            \n","            all_tables.append({\n","                'lakehouse': lh, \n","                'table': table_name,\n","                'physical_path': physical_path,\n","                'logger': lakehouse_logger,\n","                'workspace': workspace,\n","                'onelake_path': onelake_path\n","            })\n","            lakehouse_summary[lh]['tables'] += 1\n","\n","    print(f\"Found {len(all_tables)} tables across {len(lakehouse_summary)} lakehouse(s)\")\n","    print(f\"Processing in parallel with {MAX_PARALLEL_TABLES} workers...\")\n","    print()\n","\n","    # === PROCESS TABLES WITH THREADING ===\n","    from threading import Lock\n","    \n","    print_lock = Lock()\n","    results = {'success':0, 'skipped':0, 'failed':0, 'validation_failed':0, 'dry-run':0}\n","    all_results = []\n","    start = time.time()\n","    counter = {'count': 0}\n","\n","    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_TABLES) as exec:\n","        futures = {exec.submit(process_table, t, timestamp, MODE): t for t in all_tables}\n","        \n","        for f in as_completed(futures):\n","            info = futures[f]\n","            res = f.result()\n","            lh = info['lakehouse']\n","            tbl = info['table']\n","            lakehouse_logger = info['logger']\n","            \n","            with print_lock:\n","                counter['count'] += 1\n","                i = counter['count']\n","                \n","                all_results.append({\n","                    'sort_key': f\"{lh}|||{tbl}\",\n","                    'lakehouse': lh,\n","                    'table': tbl,\n","                    'result': res,\n","                    'index': i,\n","                    'total': len(all_tables)\n","                })\n","                \n","                eta = ((time.time() - start) / i) * (len(all_tables) - i) / 60\n","                \n","                # Log to both console AND lakehouse log file\n","                progress_msg = f\"[{i}/{len(all_tables)}] Completed: {lh}.{tbl} | ETA: {eta:.1f}m\"\n","                print(progress_msg)\n","                lakehouse_logger.info(progress_msg)\n","                \n","                # Log the detailed results to the lakehouse log\n","                lakehouse_logger.info(f\"--- Result for {tbl} ---\")\n","                for detail in res['details']:\n","                    lakehouse_logger.info(f\"  {detail}\")\n","                lakehouse_logger.info(\"\")\n","                \n","                results[res['status']] += 1\n","                lakehouse_summary[lh][res['status']] += 1\n","\n","    # === SORT AND DISPLAY RESULTS ===\n","    print()\n","    print(\"=\"*80)\n","    print(\"RESULTS (sorted by lakehouse and table name)\")\n","    print(\"=\"*80)\n","    print()\n","\n","    all_results.sort(key=lambda x: x['sort_key'])\n","\n","    current_lakehouse = None\n","\n","    for item in all_results:\n","        lh = item['lakehouse']\n","        tbl = item['table']\n","        res = item['result']\n","        \n","        if lh != current_lakehouse:\n","            if current_lakehouse is not None:\n","                print()\n","            \n","            header = f\"üì¶ LAKEHOUSE: {lh}\"\n","            separator = \"-\" * 80\n","            \n","            print(header)\n","            print(separator)\n","            \n","            current_lakehouse = lh\n","        \n","        # Print to console\n","        print(f\"  [{item['index']}/{item['total']}] {tbl}\")\n","        for detail in res['details']:\n","            print(f\"    {detail}\")\n","        print()\n","\n","    # === PER-LAKEHOUSE SUMMARY ===\n","    print(\"=\"*80)\n","    print(\"SUMMARY BY LAKEHOUSE\")\n","    print(\"=\"*80)\n","\n","    for lh, stats in lakehouse_summary.items():\n","        summary_header = f\"üì¶ {lh}\"\n","        print(summary_header)\n","        \n","        lh_logger = get_lakehouse_logger(lh)\n","        lh_logger.info(\"=\"*80)\n","        lh_logger.info(\"SUMMARY\")\n","        lh_logger.info(\"=\"*80)\n","        \n","        for k in ['tables', 'success', 'skipped', 'failed', 'validation_failed', 'dry-run']:\n","            if k in stats and stats[k] > 0:\n","                stat_line = f\"   {k}: {stats[k]}\"\n","                print(stat_line)\n","                lh_logger.info(stat_line)\n","        print()\n","        lh_logger.info(\"\")\n","    # ‚Üê for loop ends here\n","\n","    # ‚úÖ These lines must be at THIS indentation (4 spaces - inside else block)\n","    print(f\"‚è±Ô∏è TOTAL TIME: {time.time()-start:.1f}s\")\n","\n","    flush_logs_to_lakehouse(timestamp)\n","    print(f\"üìÅ Logs saved: Files/logs/{workspace}/[lakehouse]/[date]/\")\n","\n","    if MODE == \"apply\":\n","        print(f\"\\nüìÑ To rollback these changes:\")\n","        print(f\"   MODE = 'rollback'\")\n","        print(f\"   ROLLBACK_TIMESTAMP = '{timestamp}'\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aceb48d4-25b3-4cd2-a428-96ba620b28ef"}],"metadata":{"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5d1744ec-ab69-4918-b76d-f94e78bb1bed"}],"default_lakehouse":"5d1744ec-ab69-4918-b76d-f94e78bb1bed","default_lakehouse_name":"Lakehouse_Data","default_lakehouse_workspace_id":"c56568fc-b9dd-4299-8076-8f41d31586f5"}}},"nbformat":4,"nbformat_minor":5}